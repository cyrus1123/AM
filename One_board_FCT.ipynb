{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOw60o5ruA6+us55qITaIQ0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cyrus1123/AM/blob/main/One_board_FCT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdb\n"
      ],
      "metadata": {
        "id": "txd243OFLw2z"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "SkeKX6hwj1C4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = pd.DataFrame(np.random.random((4000, 121)))\n",
        "c = ['X_{}'.format(i) for i in range(120)]\n",
        "c.append('Y')\n",
        "a.columns = c\n",
        "a.to_excel('data.xlsx')"
      ],
      "metadata": {
        "id": "82oirI1dj4OW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install  transformers\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQVLYwj3e1pP",
        "outputId": "1b4e85c1-d99b-4455-93a5-8c39c290dff0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.35.0\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BigBirdTokenizer, BigBirdForSequenceClassification\n",
        "from torch.optim import AdamW  # This line replaces the transformers import\n",
        "\n",
        "# Specify the name of the pre-trained BigBird model you want to use\n",
        "model_name = \"google/bigbird-roberta-base\"\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = BigBirdTokenizer.from_pretrained(model_name)\n",
        "\n",
        "class LargeTextFileDataset(Dataset):\n",
        "    def __init__(self, tokenizer, file_path, labels_path):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.file_path = file_path\n",
        "        self.labels_path = labels_path\n",
        "        self.labels = self._read_labels(labels_path)\n",
        "\n",
        "    def _read_labels(self, labels_path):\n",
        "        with open(labels_path, 'r') as f:\n",
        "            labels = [int(label.strip()) for label in f]\n",
        "        return labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]  # This should fetch the text from your dataset\n",
        "        label = self.labels[idx]  # This should fetch the label from your dataset\n",
        "\n",
        "        # Tokenize the text\n",
        "        inputs = self.tokenizer(\n",
        "            text,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}  # Make sure it's not returning a batch of size 1\n",
        "\n",
        "        # Add the label\n",
        "        inputs['labels'] = torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "        return inputs\n",
        "\n",
        "#****************************************************************************************\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class ExcelDataset(Dataset):\n",
        "    def __init__(self, tokenizer, features, labels):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = ' '.join(map(str, self.features.iloc[idx, :]))\n",
        "        inputs = self.tokenizer(text, padding='max_length', truncation=True, return_tensors=\"pt\", max_length=512)\n",
        "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
        "        inputs[\"labels\"] = torch.tensor(self.labels.iloc[idx], dtype=torch.long)\n",
        "        return text\n",
        "\n",
        "# Load the data from the Excel file\n",
        "dataframe = pd.read_excel('data.xlsx')\n",
        "features = dataframe.iloc[:, :-1]  # Assuming the last column is the label\n",
        "labels = dataframe.iloc[:, -1]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "features_train, features_test, labels_train, labels_test = train_test_split(\n",
        "    features, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create the datasets\n",
        "train_dataset = ExcelDataset(tokenizer, features_train, labels_train)\n",
        "test_dataset = ExcelDataset(tokenizer, features_test, labels_test)\n",
        "\n",
        "# Define the DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "#*****************************************************************************************\n",
        "# # Define your file paths\n",
        "# train_file_path = \"/content/train.txt\"\n",
        "# train_labels_path = \"/content/train_label.txt\"\n",
        "# test_file_path = \"/content/test.txt\"\n",
        "# test_labels_path = \"/content/test_label.txt\"\n",
        "\n",
        "# # Create datasets\n",
        "# train_dataset = LargeTextFileDataset(tokenizer, train_file_path, train_labels_path)\n",
        "# test_dataset = LargeTextFileDataset(tokenizer, test_file_path, test_labels_path)\n",
        "\n",
        "# # Define the dataloaders\n",
        "# train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)  # Adjusted to 2 as per system recommendation\n",
        "# test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=2)\n",
        "\n",
        "# ... rest of the training code remains the same ...\n"
      ],
      "metadata": {
        "id": "_gUN98aZoBpP"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#safe"
      ],
      "metadata": {
        "id": "GJU1s-X4oAa1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "7RHEUQTl62NE",
        "outputId": "cc8bb295-0a63-4f94-b610-575c04b401e0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-bece606d031c>\u001b[0m in \u001b[0;36m<cell line: 54>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# Create datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLargeTextFileDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLargeTextFileDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-69-bece606d031c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tokenizer, file_path, labels_path)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-69-bece606d031c>\u001b[0m in \u001b[0;36m_read_labels\u001b[0;34m(self, labels_path)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/train_label.txt'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BigBirdTokenizer, BigBirdForSequenceClassification\n",
        "from torch.optim import AdamW  # This line replaces the transformers import\n",
        "\n",
        "# Specify the name of the pre-trained BigBird model you want to use\n",
        "model_name = \"google/bigbird-roberta-base\"\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = BigBirdTokenizer.from_pretrained(model_name)\n",
        "\n",
        "class LargeTextFileDataset(Dataset):\n",
        "    def __init__(self, tokenizer, file_path, labels_path):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.file_path = file_path\n",
        "        self.labels_path = labels_path\n",
        "        self.labels = self._read_labels(labels_path)\n",
        "\n",
        "    def _read_labels(self, labels_path):\n",
        "        with open(labels_path, 'r') as f:\n",
        "            labels = [int(label.strip()) for label in f]\n",
        "        return labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        with open(self.file_path, 'r') as f:\n",
        "            for _ in range(idx):\n",
        "                f.readline()\n",
        "            text = f.readline().strip()\n",
        "\n",
        "        label = self.labels[idx]\n",
        "        inputs = self.tokenizer(text, padding='max_length', truncation=True, return_tensors=\"pt\", max_length=512)\n",
        "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
        "        inputs[\"labels\"] = torch.tensor(label, dtype=torch.long)\n",
        "        return inputs\n",
        "\n",
        "# Define your file paths\n",
        "train_file_path = \"/content/train.txt\"\n",
        "train_labels_path = \"/content/train_label.txt\"\n",
        "test_file_path = \"/content/test.txt\"\n",
        "test_labels_path = \"/content/test_label.txt\"\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = LargeTextFileDataset(tokenizer, train_file_path, train_labels_path)\n",
        "test_dataset = LargeTextFileDataset(tokenizer, test_file_path, test_labels_path)\n",
        "\n",
        "# Define the dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)  # Adjusted to 2 as per system recommendation\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=2)\n",
        "\n",
        "# ... rest of the training code remains the same ...\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    batch_input_ids = [item['input_ids'] for item in batch]\n",
        "    batch_attention_mask = [item['attention_mask'] for item in batch]\n",
        "    batch_labels = [item['labels'] for item in batch]\n",
        "\n",
        "    # Convert lists to tensors\n",
        "    batch_input_ids = torch.stack(batch_input_ids)\n",
        "    batch_attention_mask = torch.stack(batch_attention_mask)\n",
        "    batch_labels = torch.stack(batch_labels)\n",
        "\n",
        "    return {\n",
        "        'input_ids': batch_input_ids,\n",
        "        'attention_mask': batch_attention_mask,\n",
        "        'labels': batch_labels\n",
        "    }\n"
      ],
      "metadata": {
        "id": "51d3zVX7YtaF"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "Zxjl23YuZhW5"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from torch.utils.data import DataLoader\n",
        "# from transformers import BigBirdTokenizer, BigBirdForSequenceClassification, AdamW\n",
        "\n",
        "# Initialize tokenizer and model\n",
        "model_name = \"google/bigbird-roberta-base\"\n",
        "tokenizer = BigBirdTokenizer.from_pretrained(model_name)\n",
        "model = BigBirdForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "\n",
        "# Trainer class\n",
        "class FCT_Trainer:\n",
        "    def __init__(self, learning_rate, per_device_train_batch_size, per_device_eval_batch_size, n_epochs, weight_decay):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.per_device_train_batch_size = per_device_train_batch_size\n",
        "        self.per_device_eval_batch_size = per_device_eval_batch_size\n",
        "        self.n_epochs = n_epochs\n",
        "        self.weight_decay = weight_decay\n",
        "\n",
        "    def train(self, model, train_loader, test_loader, epochs=None):\n",
        "        optimizer = AdamW(model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
        "        epochs = epochs or self.n_epochs\n",
        "        gradient_accumulation_steps = 4  # Set this to the number of accumulation steps you want\n",
        "        model.zero_grad()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            model.train()\n",
        "            total_loss = 0\n",
        "            for step, batch in enumerate(train_loader):\n",
        "                inputs = {key: value for key, value in batch.items() if key != \"labels\"}\n",
        "                labels = batch[\"labels\"]\n",
        "                outputs = model(**inputs, labels=labels)\n",
        "                loss = outputs.loss / gradient_accumulation_steps\n",
        "                loss.backward()\n",
        "\n",
        "                if (step + 1) % gradient_accumulation_steps == 0:\n",
        "                    optimizer.step()\n",
        "                    model.zero_grad()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            avg_train_loss = total_loss / (len(train_loader) / gradient_accumulation_steps)\n",
        "            print(f'Epoch {epoch+1}, Training Loss: {avg_train_loss:.3f}')\n",
        "\n",
        "            # Saving the model after every epoch\n",
        "            torch.save(model.state_dict(), f'model_checkpoint_epoch_{epoch}.pt')\n",
        "\n",
        "            # Evaluation (if needed)\n",
        "            # ...\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = FCT_Trainer(\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    n_epochs=3,\n",
        "    weight_decay=0.01\n",
        ")\n",
        "\n",
        "# Start the training process\n",
        "trainer.train(model, train_loader, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "-rGLLeGuhzaE",
        "outputId": "b4a7bf73-700e-4a49-ab33-07e1ec5d9abc"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-58ce76fe82b9>\u001b[0m in \u001b[0;36m<cell line: 62>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m# Start the training process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-74-58ce76fe82b9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model, train_loader, test_loader, epochs)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-72-a41f4f5d7876>\u001b[0m in \u001b[0;36mcollate_fn\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mbatch_input_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mbatch_attention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbatch_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-72-a41f4f5d7876>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mbatch_input_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mbatch_attention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbatch_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2"
      ],
      "metadata": {
        "id": "4lWrr2aTiuIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BigBirdTokenizer, BigBirdForSequenceClassification\n",
        "\n",
        "# Create a small example to simulate the process\n",
        "model_name = \"google/bigbird-roberta-base\"\n",
        "tokenizer = BigBirdTokenizer.from_pretrained(model_name)\n",
        "model = BigBirdForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Get the embedding layer's weights\n",
        "embedding_weight = None\n",
        "for name, param in model.named_parameters():\n",
        "    if 'embeddings' in name and 'weight' in name:  # targeting the embedding layer's weights\n",
        "        embedding_weight = param.data\n",
        "        break\n",
        "\n",
        "# Simulate a small batch of inputs\n",
        "texts = [\"example sentence for testing\", \"another example sentence\"]\n",
        "inputs = tokenizer(texts, padding='max_length', truncation=True, return_tensors=\"pt\", max_length=128)\n",
        "\n",
        "# Extract the input_ids from the inputs\n",
        "input_ids = inputs['input_ids']\n",
        "\n",
        "# Perform the embedding lookup\n",
        "input_embeddings = torch.nn.functional.embedding(input_ids, embedding_weight)\n",
        "\n",
        "# Check the shapes to confirm the operation was successful\n",
        "input_ids_shape = input_ids.shape\n",
        "input_embeddings_shape = input_embeddings.shape\n",
        "print(input_ids_shape, input_embeddings_shape)\n",
        "\n",
        "\n",
        "A_expanded = input_ids.unsqueeze(-1)\n",
        "\n",
        "A_expanded * input_embeddings\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzPrXhuciv5Q",
        "outputId": "f2f32840-4e76-4b16-a007-65332c148400"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 128]) torch.Size([2, 128, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A_expanded = input_ids.unsqueeze(-1)\n",
        "\n",
        "A_expanded * input_embeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bha_fxmZi7zV",
        "outputId": "a5d55502-97b7-42db-e790-11ce5490cc65"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 6.6826e+00, -1.9162e+00,  1.2326e+00,  ..., -4.0563e-01,\n",
              "           2.4483e+00,  1.5788e+00],\n",
              "         [ 1.0695e+02, -1.4199e+01, -1.8223e+02,  ...,  2.2165e+01,\n",
              "           5.2086e+01,  1.0938e+02],\n",
              "         [-5.2572e+02,  4.8808e+02, -1.1746e+03,  ..., -2.2947e+02,\n",
              "          -3.2555e+02, -3.9253e+02],\n",
              "         ...,\n",
              "         [ 0.0000e+00, -0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
              "           0.0000e+00,  0.0000e+00],\n",
              "         [ 0.0000e+00, -0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
              "           0.0000e+00,  0.0000e+00],\n",
              "         [ 0.0000e+00, -0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
              "           0.0000e+00,  0.0000e+00]],\n",
              "\n",
              "        [[ 6.6826e+00, -1.9162e+00,  1.2326e+00,  ..., -4.0563e-01,\n",
              "           2.4483e+00,  1.5788e+00],\n",
              "         [-9.2521e+01,  5.1514e+01, -2.2008e+01,  ...,  1.3889e+02,\n",
              "           3.2781e+01, -3.7696e+01],\n",
              "         [ 1.0695e+02, -1.4199e+01, -1.8223e+02,  ...,  2.2165e+01,\n",
              "           5.2086e+01,  1.0938e+02],\n",
              "         ...,\n",
              "         [ 0.0000e+00, -0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
              "           0.0000e+00,  0.0000e+00],\n",
              "         [ 0.0000e+00, -0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
              "           0.0000e+00,  0.0000e+00],\n",
              "         [ 0.0000e+00, -0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
              "           0.0000e+00,  0.0000e+00]]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3"
      ],
      "metadata": {
        "id": "RUlWzDY2bqIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    import torch\n",
        "    from torch.utils.data import DataLoader, Dataset\n",
        "    from transformers import BigBirdTokenizer, BigBirdForSequenceClassification\n",
        "    from torch.optim import AdamW  # This line replaces the transformers import\n",
        "\n",
        "    # Specify the name of the pre-trained BigBird model you want to use\n",
        "    model_name = \"google/bigbird-roberta-base\"\n",
        "\n",
        "    # Load the tokenizer\n",
        "    tokenizer = BigBirdTokenizer.from_pretrained(model_name)\n",
        "\n",
        "class LargeTextFileDataset(Dataset):\n",
        "  def __init__(self, tokenizer, file_path, labels_path):\n",
        "      self.tokenizer = tokenizer\n",
        "      self.file_path = file_path\n",
        "      self.labels_path = labels_path\n",
        "      self.labels = self._read_labels(labels_path)\n",
        "      self.texts = self._read_texts(file_path)\n",
        "\n",
        "  def _read_labels(self, labels_path):\n",
        "      with open(labels_path, 'r') as f:\n",
        "      labels = [int(label.strip()) for label in f]\n",
        "      return labels\n",
        "\n",
        "\n",
        "  def _read_texts(self, file_path):\n",
        "  # This method should be implemented to read the texts from the file\n",
        "  # Here's a placeholder for file reading, it should be adapted to your specific file format\n",
        "      with open(file_path, 'r', encoding='utf-8') as f:\n",
        "      texts = [line.strip() for line in f]\n",
        "      return texts\n",
        "  def __len__(self):\n",
        "      return len(self.labels)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      text = self.texts[idx]  # This should fetch the text from your dataset\n",
        "      label = self.labels[idx]  # This should fetch the label from your dataset\n",
        "\n",
        "      # Tokenize the text\n",
        "      inputs = self.tokenizer(\n",
        "      text,\n",
        "      padding='max_length',\n",
        "      truncation=True,\n",
        "      max_length=512,\n",
        "      return_tensors=\"pt\"\n",
        "      )\n",
        "      inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
        "      # Add the label to the inputs dictionary\n",
        "      inputs['labels'] = torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "      return inputs\n",
        "      # Make sure it's not returning a batch of size 1\n",
        "\n",
        "      # Add the label\n",
        "      inputs['labels'] = torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "      return inputs\n",
        "\n",
        "    #****************************************************************************************\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class ExcelDataset(Dataset):\n",
        "  def __init__(self, tokenizer, features, labels):\n",
        "      self.tokenizer = tokenizer\n",
        "      self.features = features\n",
        "      self.labels = labels\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.labels)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      text = ' '.join(map(str, self.features.iloc[idx, :]))\n",
        "      inputs = self.tokenizer(text, padding='max_length', truncation=True, return_tensors=\"pt\", max_length=512)\n",
        "      inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
        "      inputs[\"labels\"] = torch.tensor(self.labels.iloc[idx], dtype=torch.long)\n",
        "      return text\n",
        "\n",
        "      # Load the data from the Excel file\n",
        "      dataframe = pd.read_excel('data.xlsx')\n",
        "      features = dataframe.iloc[:, :-1]  # Assuming the last column is the label\n",
        "      labels = dataframe.iloc[:, -1]\n",
        "\n",
        "      # Split the data into training and testing sets\n",
        "      features_train, features_test, labels_train, labels_test = train_test_split(\n",
        "      features, labels, test_size=0.2, random_state=42\n",
        "      )\n",
        "\n",
        "      # Create the datasets\n",
        "      train_dataset = ExcelDataset(tokenizer, features_train, labels_train)\n",
        "      test_dataset = ExcelDataset(tokenizer, features_test, labels_test)\n",
        "\n",
        "      # Define the DataLoaders\n",
        "      train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "      test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "      #*****************************************************************************************\n",
        "      # # Define your file paths\n",
        "      # train_file_path = \"/content/train.txt\"\n",
        "      # train_labels_path = \"/content/train_label.txt\"\n",
        "      # test_file_path = \"/content/test.txt\"\n",
        "      # test_labels_path = \"/content/test_label.txt\"\n",
        "\n",
        "      # # Create datasets\n",
        "      # train_dataset = LargeTextFileDataset(tokenizer, train_file_path, train_labels_path)\n",
        "      # test_dataset = LargeTextFileDataset(tokenizer, test_file_path, test_labels_path)\n",
        "\n",
        "      # # Define the dataloaders\n",
        "      # train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)  # Adjusted to 2 as per system recommendation\n",
        "      # test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=2)\n",
        "\n",
        "      # ... rest of the training code remains the same ...\n",
        "\n",
        "\n",
        "\n",
        "  def collate_fn(batch):\n",
        "      batch_input_ids = [item['input_ids'] for item in batch]\n",
        "      batch_attention_mask = [item['attention_mask'] for item in batch]\n",
        "      batch_labels = torch.stack(batch_labels)\n",
        "\n",
        "      return {\n",
        "      'input_ids': batch_input_ids,\n",
        "      'attention_mask': batch_attention_mask,\n",
        "      'labels': batch_labels\n",
        "      }\n",
        "\n",
        "\n",
        "      train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "      test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "\n",
        "\n",
        "      # import torch\n",
        "      # from torch.utils.data import DataLoader\n",
        "      # from transformers import BigBirdTokenizer, BigBirdForSequenceClassification, AdamW\n",
        "\n",
        "      # Initialize tokenizer and model\n",
        "      model_name = \"google/bigbird-roberta-base\"\n",
        "      tokenizer = BigBirdTokenizer.from_pretrained(model_name)\n",
        "      model = BigBirdForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "\n",
        "    # Trainer class\n",
        "class FCT_Trainer:\n",
        "  def __init__(self, learning_rate, per_device_train_batch_size, per_device_eval_batch_size, n_epochs, weight_decay):\n",
        "      self.learning_rate = learning_rate\n",
        "      self.per_device_train_batch_size = per_device_train_batch_size\n",
        "      self.per_device_eval_batch_size = per_device_eval_batch_size\n",
        "      self.n_epochs = n_epochs\n",
        "      self.weight_decay = weight_decay\n",
        "\n",
        "  def train(self, model, train_loader, test_loader, epochs=None):\n",
        "      optimizer = AdamW(model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
        "      epochs = epochs or self.n_epochs\n",
        "      gradient_accumulation_steps = 4  # Set this to the number of accumulation steps you want\n",
        "      model.zero_grad()\n",
        "\n",
        "      for epoch in range(epochs):\n",
        "      model.train()\n",
        "      total_loss = 0\n",
        "      for step, batch in enumerate(train_loader):\n",
        "      inputs = {key: value for key, value in batch.items() if key != \"labels\"}\n",
        "      labels = batch[\"labels\"]\n",
        "      outputs = model(**inputs, labels=labels)\n",
        "      loss = outputs.loss / gradient_accumulation_steps\n",
        "      loss.backward()\n",
        "\n",
        "      if (step + 1) % gradient_accumulation_steps == 0:\n",
        "      optimizer.step()\n",
        "      model.zero_grad()\n",
        "\n",
        "      total_loss += loss.item()\n",
        "\n",
        "      avg_train_loss = total_loss / (len(train_loader) / gradient_accumulation_steps)\n",
        "      print(f'Epoch {epoch+1}, Training Loss: {avg_train_loss:.3f}')\n",
        "\n",
        "      # Saving the model after every epoch\n",
        "      torch.save(model.state_dict(), f'model_checkpoint_epoch_{epoch}.pt')\n",
        "\n",
        "      # Evaluation (if needed)\n",
        "      # ...\n",
        "\n",
        "      # Initialize trainer\n",
        "      trainer = FCT_Trainer(\n",
        "      learning_rate=5e-5,\n",
        "      per_device_train_batch_size=2,\n",
        "      per_device_eval_batch_size=2,\n",
        "      n_epochs=3,\n",
        "      weight_decay=0.01\n",
        "      )\n",
        "\n",
        "      # Start the training process\n",
        "      trainer.train(model, train_loader, test_loader)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "CC-KNz4ybroq",
        "outputId": "5f0542d3-685c-4d3e-8b5b-8a65b7b80a1a"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-86-a71c191d90d8>\"\u001b[0;36m, line \u001b[0;32m114\u001b[0m\n\u001b[0;31m    trainer = FCT_Trainer(\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after class definition on line 110\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#last working"
      ],
      "metadata": {
        "id": "azf0J58kmz25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BigBirdTokenizer, BigBirdForSequenceClassification\n",
        "from torch.optim import AdamW  # This line replaces the transformers import\n",
        "\n",
        "# Specify the name of the pre-trained BigBird model you want to use\n",
        "model_name = \"google/bigbird-roberta-base\"\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = BigBirdTokenizer.from_pretrained(model_name)\n",
        "\n",
        "class LargeTextFileDataset(Dataset):\n",
        "    def __init__(self, tokenizer, file_path, labels_path):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.file_path = file_path\n",
        "        self.labels_path = labels_path\n",
        "        self.labels = self._read_labels(labels_path)\n",
        "        self.texts = self._read_texts(file_path)\n",
        "\n",
        "    def _read_labels(self, labels_path):\n",
        "        with open(labels_path, 'r') as f:\n",
        "            labels = [int(label.strip()) for label in f]\n",
        "        return labels\n",
        "\n",
        "    def _read_texts(self, file_path):\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            texts = [line.strip() for line in f]\n",
        "        return texts\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]  # This should fetch the text from your dataset\n",
        "        label = self.labels[idx]  # This should fetch the label from your dataset\n",
        "\n",
        "        # Tokenize the text\n",
        "        inputs = self.tokenizer(\n",
        "            text,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
        "        # Add the label to the inputs dictionary\n",
        "        inputs['labels'] = torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "        return inputs\n",
        "\n",
        "#****************************************************************************************\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "class ExcelDataset(Dataset):\n",
        "    def __init__(self, tokenizer, features, labels):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = ' '.join(map(str, self.features.iloc[idx, :]))\n",
        "        inputs = self.tokenizer(text, padding='max_length', truncation=True, return_tensors=\"pt\", max_length=512)\n",
        "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
        "        inputs[\"labels\"] = torch.tensor(self.labels.iloc[idx], dtype=torch.long)\n",
        "        return inputs\n",
        "\n",
        "# Load the data from the Excel file\n",
        "dataframe = pd.read_excel('data.xlsx')\n",
        "features = dataframe.iloc[:, :-1]  # Assuming the last column is the label\n",
        "labels = dataframe.iloc[:, -1]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "features_train, features_test, labels_train, labels_test = train_test_split(\n",
        "    features, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create the datasets\n",
        "train_dataset = ExcelDataset(tokenizer, features_train, labels_train)\n",
        "test_dataset = ExcelDataset(tokenizer, features_test, labels_test)\n",
        "\n",
        "# Define the DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "#*****************************************************************************************\n",
        "\n",
        "# Define the collate_fn\n",
        "def collate_fn(batch):\n",
        "    batch_input_ids = [item['input_ids'] for item in batch]\n",
        "    batch_attention_mask = [item['attention_mask'] for item in batch]\n",
        "    batch_labels = [item['labels'] for item in batch]\n",
        "\n",
        "    return {\n",
        "        'input_ids': torch.stack(batch_input_ids),\n",
        "        'attention_mask': torch.stack(batch_attention_mask),\n",
        "        'labels': torch.stack(batch_labels)\n",
        "    }\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Initialize model\n",
        "model = BigBirdForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Trainer class\n",
        "class FCT_Trainer:\n",
        "  def __init__(self, learning_rate, per_device_train_batch_size, per_device_eval_batch_size, n_epochs, weight_decay):\n",
        "      self.learning_rate = learning_rate\n",
        "      self.per_device_train_batch_size = per_device_train_batch_size\n",
        "      self.per_device_eval_batch_size = per_device_eval_batch_size\n",
        "      self.n_epochs = n_epochs\n",
        "      self.weight_decay = weight_decay\n",
        "\n",
        "  def train(self, model, train_loader, test_loader, epochs=None):\n",
        "      optimizer = AdamW(model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
        "      epochs = epochs or self.n_epochs\n",
        "      gradient_accumulation_steps = 4  # Set this to the number of accumulation steps you want\n",
        "      model.zero_grad()\n",
        "\n",
        "      for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for step, batch in enumerate(train_loader):\n",
        "          inputs = {key: value for key, value in batch.items() if key != \"labels\"}\n",
        "          labels = batch[\"labels\"]\n",
        "          outputs = model(**inputs, labels=labels)\n",
        "          loss = outputs.loss / gradient_accumulation_steps\n",
        "          loss.backward()\n",
        "\n",
        "          if (step + 1) % gradient_accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            model.zero_grad()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            avg_train_loss = total_loss / (len(train_loader) / gradient_accumulation_steps)\n",
        "            print(f'Epoch {epoch+1}, Training Loss: {avg_train_loss:.3f}')\n",
        "\n",
        "        # Saving the model after every epoch\n",
        "      torch.save(model.state_dict(), f'model_checkpoint_epoch_{epoch}.pt')\n",
        "\n",
        "      # Evaluation (if needed)\n",
        "      # ...\n",
        "# Initialize trainer\n",
        "trainer = FCT_Trainer(\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    n_epochs=3,\n",
        "    weight_decay=0.01\n",
        ")\n",
        "\n",
        "# Start the training process\n",
        "trainer.train(model, train_loader, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xaOk6yyddkp3",
        "outputId": "1cafe610-e79a-4070-c419-ba65b9bf78c1"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Attention type 'block_sparse' is not possible if sequence_length: 512 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training Loss: 0.001\n",
            "Epoch 1, Training Loss: 0.001\n",
            "Epoch 1, Training Loss: 0.001\n",
            "Epoch 1, Training Loss: 0.001\n",
            "Epoch 1, Training Loss: 0.001\n",
            "Epoch 1, Training Loss: 0.001\n",
            "Epoch 1, Training Loss: 0.001\n",
            "Epoch 1, Training Loss: 0.001\n",
            "Epoch 1, Training Loss: 0.001\n",
            "Epoch 1, Training Loss: 0.002\n",
            "Epoch 1, Training Loss: 0.002\n",
            "Epoch 1, Training Loss: 0.002\n",
            "Epoch 1, Training Loss: 0.002\n",
            "Epoch 1, Training Loss: 0.002\n",
            "Epoch 1, Training Loss: 0.002\n",
            "Epoch 1, Training Loss: 0.002\n",
            "Epoch 1, Training Loss: 0.002\n",
            "Epoch 1, Training Loss: 0.002\n",
            "Epoch 1, Training Loss: 0.002\n",
            "Epoch 1, Training Loss: 0.002\n",
            "Epoch 1, Training Loss: 0.002\n",
            "Epoch 1, Training Loss: 0.002\n",
            "Epoch 1, Training Loss: 0.002\n",
            "Epoch 1, Training Loss: 0.002\n",
            "Epoch 1, Training Loss: 0.002\n",
            "Epoch 1, Training Loss: 0.002\n",
            "Epoch 1, Training Loss: 0.002\n",
            "Epoch 1, Training Loss: 0.002\n",
            "Epoch 1, Training Loss: 0.002\n",
            "Epoch 1, Training Loss: 0.002\n",
            "Epoch 1, Training Loss: 0.002\n",
            "Epoch 1, Training Loss: 0.002\n",
            "Epoch 1, Training Loss: 0.002\n",
            "Epoch 1, Training Loss: 0.002\n",
            "Epoch 1, Training Loss: 0.002\n",
            "Epoch 1, Training Loss: 0.002\n",
            "Epoch 1, Training Loss: 0.002\n",
            "Epoch 1, Training Loss: 0.002\n",
            "Epoch 1, Training Loss: 0.002\n",
            "Epoch 1, Training Loss: 0.002\n",
            "Epoch 1, Training Loss: 0.002\n",
            "Epoch 1, Training Loss: 0.002\n",
            "Epoch 1, Training Loss: 0.002\n",
            "Epoch 1, Training Loss: 0.002\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-694fe6c6525e>\u001b[0m in \u001b[0;36m<cell line: 158>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;31m# Start the training process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-88-694fe6c6525e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model, train_loader, test_loader, epochs)\u001b[0m\n\u001b[1;32m    128\u001b[0m           \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m           \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m           \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m           \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/big_bird/modeling_big_bird.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   2747\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2749\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   2750\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2751\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/big_bird/modeling_big_bird.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   2135\u001b[0m         )\n\u001b[1;32m   2136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2137\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   2138\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2139\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/big_bird/modeling_big_bird.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, band_mask, from_mask, to_mask, blocked_encoder_mask, return_dict)\u001b[0m\n\u001b[1;32m   1633\u001b[0m                 )\n\u001b[1;32m   1634\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1635\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m   1636\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1637\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/big_bird/modeling_big_bird.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, band_mask, from_mask, to_mask, blocked_encoder_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m   1490\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1491\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1492\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m   1493\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/big_bird/modeling_big_bird.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, band_mask, from_mask, to_mask, from_blocked_mask, to_blocked_mask)\u001b[0m\n\u001b[1;32m   1391\u001b[0m             \u001b[0mto_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"original_full\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1393\u001b[0;31m             self_outputs = self.self(\n\u001b[0m\u001b[1;32m   1394\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1395\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/big_bird/modeling_big_bird.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0mattention_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_probs\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0mcontext_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BigBirdTokenizer, BigBirdForSequenceClassification\n",
        "\n",
        "# Create a small example to simulate the process\n",
        "model_name = \"google/bigbird-roberta-base\"\n",
        "tokenizer = BigBirdTokenizer.from_pretrained(model_name)\n",
        "model = BigBirdForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Get the embedding layer's weights\n",
        "embedding_weight = None\n",
        "for name, param in model.named_parameters():\n",
        "    if 'embeddings' in name and 'weight' in name:  # targeting the embedding layer's weights\n",
        "        embedding_weight = param.data\n",
        "        break\n",
        "\n",
        "# Simulate a small batch of inputs\n",
        "texts = [\"example sentence for testing\", \"another example sentence\"]\n",
        "inputs = tokenizer(texts, padding='max_length', truncation=True, return_tensors=\"pt\", max_length=128)\n",
        "\n",
        "# Extract the input_ids from the inputs\n",
        "input_ids = inputs['input_ids']\n",
        "\n",
        "# Perform the embedding lookup\n",
        "input_embeddings = torch.nn.functional.embedding(input_ids, embedding_weight)\n",
        "\n",
        "# Check the shapes to confirm the operation was successful\n",
        "input_ids_shape = input_ids.shape\n",
        "input_embeddings_shape = input_embeddings.shape\n",
        "print(input_ids_shape, input_embeddings_shape)\n",
        "\n",
        "\n",
        "A_expanded = input_ids.unsqueeze(-1)\n",
        "\n",
        "A_expanded * input_embeddings\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JJmYoeAmtD1",
        "outputId": "856d50ab-2647-4385-810c-bccb18f5ae20"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 128]) torch.Size([2, 128, 768])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 6.6826e+00, -1.9162e+00,  1.2326e+00,  ..., -4.0563e-01,\n",
              "           2.4483e+00,  1.5788e+00],\n",
              "         [ 1.0695e+02, -1.4199e+01, -1.8223e+02,  ...,  2.2165e+01,\n",
              "           5.2086e+01,  1.0938e+02],\n",
              "         [-5.2572e+02,  4.8808e+02, -1.1746e+03,  ..., -2.2947e+02,\n",
              "          -3.2555e+02, -3.9253e+02],\n",
              "         ...,\n",
              "         [ 0.0000e+00, -0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
              "           0.0000e+00,  0.0000e+00],\n",
              "         [ 0.0000e+00, -0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
              "           0.0000e+00,  0.0000e+00],\n",
              "         [ 0.0000e+00, -0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
              "           0.0000e+00,  0.0000e+00]],\n",
              "\n",
              "        [[ 6.6826e+00, -1.9162e+00,  1.2326e+00,  ..., -4.0563e-01,\n",
              "           2.4483e+00,  1.5788e+00],\n",
              "         [-9.2521e+01,  5.1514e+01, -2.2008e+01,  ...,  1.3889e+02,\n",
              "           3.2781e+01, -3.7696e+01],\n",
              "         [ 1.0695e+02, -1.4199e+01, -1.8223e+02,  ...,  2.2165e+01,\n",
              "           5.2086e+01,  1.0938e+02],\n",
              "         ...,\n",
              "         [ 0.0000e+00, -0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
              "           0.0000e+00,  0.0000e+00],\n",
              "         [ 0.0000e+00, -0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
              "           0.0000e+00,  0.0000e+00],\n",
              "         [ 0.0000e+00, -0.0000e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
              "           0.0000e+00,  0.0000e+00]]])"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    }
  ]
}