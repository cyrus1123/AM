{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cyrus1123/AM/blob/main/plug_and_play1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txd243OFLw2z"
      },
      "outputs": [],
      "source": [
        "import pdb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SkeKX6hwj1C4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82oirI1dj4OW"
      },
      "outputs": [],
      "source": [
        "# a = pd.DataFrame(np.random.random((4000, 121)))\n",
        "# c = ['X_{}'.format(i) for i in range(120)]\n",
        "# c.append('Y')\n",
        "# a.columns = c\n",
        "# a.to_excel('data.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQVLYwj3e1pP",
        "outputId": "8ba9c711-d76e-4110-d688-7869d2843fb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.35.0\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ],
      "source": [
        "!pip install  transformers\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azf0J58kmz25"
      },
      "source": [
        "#last working"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaOk6yyddkp3",
        "outputId": "6463b8fb-8f6f-4cdc-ab47-35206cdf7e54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Attention type 'block_sparse' is not possible if sequence_length: 512 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training Loss: 0.0004377116 0.17508463561534882\n",
            "Epoch 1, Training Loss: 0.0007718250 0.30873000621795654\n",
            "Epoch 1, Training Loss: 0.0009330775 0.3732310011982918\n",
            "Epoch 1, Training Loss: 0.0010328986 0.41315943375229836\n",
            "Epoch 1, Training Loss: 0.0011735310 0.469412412494421\n",
            "Epoch 1, Training Loss: 0.0012558409 0.5023363567888737\n",
            "Epoch 1, Training Loss: 0.0012950819 0.5180327445268631\n",
            "Epoch 1, Training Loss: 0.0013461444 0.5384577792137861\n",
            "Epoch 1, Training Loss: 0.0014468051 0.5787220429629087\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BigBirdTokenizer, BigBirdForSequenceClassification\n",
        "from torch.optim import AdamW  # This line replaces the transformers import\n",
        "\n",
        "# Specify the name of the pre-trained BigBird model you want to use\n",
        "model_name = \"google/bigbird-roberta-base\"\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = BigBirdTokenizer.from_pretrained(model_name)\n",
        "\n",
        "class LargeTextFileDataset(Dataset):\n",
        "    def __init__(self, tokenizer, file_path, labels_path):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.file_path = file_path\n",
        "        self.labels_path = labels_path\n",
        "        self.labels = self._read_labels(labels_path)\n",
        "        self.texts = self._read_texts(file_path)\n",
        "\n",
        "    def _read_labels(self, labels_path):\n",
        "        with open(labels_path, 'r') as f:\n",
        "            labels = [int(label.strip()) for label in f]\n",
        "        return labels\n",
        "\n",
        "    def _read_texts(self, file_path):\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            texts = [line.strip() for line in f]\n",
        "        return texts\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]  # This should fetch the text from your dataset\n",
        "        label = self.labels[idx]  # This should fetch the label from your dataset\n",
        "\n",
        "        # Tokenize the text\n",
        "        inputs = self.tokenizer(\n",
        "            text,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
        "        # Add the label to the inputs dictionary\n",
        "        inputs['labels'] = torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "        return inputs\n",
        "\n",
        "#****************************************************************************************\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "class ExcelDataset(Dataset):\n",
        "    def __init__(self, tokenizer, features, labels):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = ' '.join(map(str, self.features.iloc[idx, :]))\n",
        "        inputs = self.tokenizer(text, padding='max_length', truncation=True, return_tensors=\"pt\", max_length=512)\n",
        "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
        "        inputs[\"labels\"] = torch.tensor(self.labels.iloc[idx], dtype=torch.long)\n",
        "        return inputs\n",
        "\n",
        "# Load the data from the Excel file\n",
        "dataframe = pd.read_excel('data.xlsx')\n",
        "features = dataframe.iloc[:, :-1]  # Assuming the last column is the label\n",
        "labels = dataframe.iloc[:, -1]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "features_train, features_test, labels_train, labels_test = train_test_split(\n",
        "    features, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create the datasets\n",
        "train_dataset = ExcelDataset(tokenizer, features_train, labels_train)\n",
        "test_dataset = ExcelDataset(tokenizer, features_test, labels_test)\n",
        "\n",
        "# Define the DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "#*****************************************************************************************\n",
        "\n",
        "# Define the collate_fn\n",
        "def collate_fn(batch):\n",
        "    batch_input_ids = [item['input_ids'] for item in batch]\n",
        "    batch_attention_mask = [item['attention_mask'] for item in batch]\n",
        "    batch_labels = [item['labels'] for item in batch]\n",
        "\n",
        "    return {\n",
        "        'input_ids': torch.stack(batch_input_ids),\n",
        "        'attention_mask': torch.stack(batch_attention_mask),\n",
        "        'labels': torch.stack(batch_labels)\n",
        "    }\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Initialize model\n",
        "model = BigBirdForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Trainer class\n",
        "class FCT_Trainer:\n",
        "  def __init__(self, learning_rate, per_device_train_batch_size, per_device_eval_batch_size, n_epochs, weight_decay):\n",
        "      self.learning_rate = learning_rate\n",
        "      self.per_device_train_batch_size = per_device_train_batch_size\n",
        "      self.per_device_eval_batch_size = per_device_eval_batch_size\n",
        "      self.n_epochs = n_epochs\n",
        "      self.weight_decay = weight_decay\n",
        "\n",
        "  def train(self, model, train_loader, test_loader, epochs=None):\n",
        "      optimizer = AdamW(model.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
        "      epochs = epochs or self.n_epochs\n",
        "      gradient_accumulation_steps = 4  # Set this to the number of accumulation steps you want\n",
        "      model.zero_grad()\n",
        "\n",
        "      for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for step, batch in enumerate(train_loader):\n",
        "          inputs = {key: value for key, value in batch.items() if key != \"labels\"}\n",
        "          labels = batch[\"labels\"]\n",
        "          outputs = model(**inputs, labels=labels)\n",
        "          loss = outputs.loss / gradient_accumulation_steps\n",
        "          loss.backward()\n",
        "\n",
        "          if (step + 1) % gradient_accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            model.zero_grad()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            avg_train_loss = total_loss / (len(train_loader) / gradient_accumulation_steps)\n",
        "            print(f'Epoch {epoch+1}, Training Loss: {avg_train_loss:.10f}', total_loss)\n",
        "\n",
        "        # Saving the model after every epoch\n",
        "      torch.save(model.state_dict(), f'model_checkpoint_epoch_{epoch}.pt')\n",
        "\n",
        "      # Evaluation (if needed)\n",
        "      # ...\n",
        "# Initialize trainer\n",
        "trainer = FCT_Trainer(\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    n_epochs=3,\n",
        "    weight_decay=0.01\n",
        ")\n",
        "\n",
        "# Start the training process\n",
        "trainer.train(model, train_loader, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JDAx3uaBvla6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAi17J1Lpd-Z"
      },
      "outputs": [],
      "source": [
        "probabilities = torch.softmax(outputs.logits, dim=1)\n",
        "predictions = torch.argmax(probabilities, dim=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JJmYoeAmtD1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BigBirdTokenizer, BigBirdForSequenceClassification\n",
        "\n",
        "# Create a small example to simulate the process\n",
        "model_name = \"google/bigbird-roberta-base\"\n",
        "tokenizer = BigBirdTokenizer.from_pretrained(model_name)\n",
        "model = BigBirdForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Get the embedding layer's weights\n",
        "embedding_weight = None\n",
        "for name, param in model.named_parameters():\n",
        "    if 'embeddings' in name and 'weight' in name:  # targeting the embedding layer's weights\n",
        "        embedding_weight = param.data\n",
        "        break\n",
        "\n",
        "# Simulate a small batch of inputs\n",
        "texts = [\"example sentence for testing\", \"another example sentence\"]\n",
        "inputs = tokenizer(texts, padding='max_length', truncation=True, return_tensors=\"pt\", max_length=128)\n",
        "\n",
        "# Extract the input_ids from the inputs\n",
        "input_ids = inputs['input_ids']\n",
        "\n",
        "# Perform the embedding lookup\n",
        "input_embeddings = torch.nn.functional.embedding(input_ids, embedding_weight)\n",
        "\n",
        "# Check the shapes to confirm the operation was successful\n",
        "input_ids_shape = input_ids.shape\n",
        "input_embeddings_shape = input_embeddings.shape\n",
        "print(input_ids_shape, input_embeddings_shape)\n",
        "\n",
        "\n",
        "A_expanded = input_ids.unsqueeze(-1)\n",
        "\n",
        "A_expanded * input_embeddings\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMmbdxCrKV8x661UnphFORD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}